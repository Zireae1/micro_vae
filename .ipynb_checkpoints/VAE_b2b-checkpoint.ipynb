{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485968ff-17e4-415f-a19a-c296908593f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import comet_ml at the top of your file\n",
    "from comet_ml import Experiment\n",
    "\n",
    "from config import *\n",
    "\n",
    "import torch\n",
    "#import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from skbio.stats.composition import clr\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:1' if use_cuda else 'mps')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd77a47-9c1b-4545-badd-e261e108d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment with your api key\n",
    "experiment = Experiment(\n",
    "    api_key=API_KEY,\n",
    "    project_name=\"vae-eco-ml\",\n",
    "    workspace=\"zireae1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56110644-7350-473a-b6cc-fa9cd45ee12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data\n",
    "data_df = pd.read_csv(\"data/wgs_train_health.noab_data_to_ml.filt.txt\", sep=\"\\t\")\n",
    "data_df = data_df.to_numpy()\n",
    "print(data_df.shape)\n",
    "\n",
    "### binarize\n",
    "zero_thr = 0\n",
    "data_df[(data_df>zero_thr)]=1\n",
    "data_df[(data_df<=zero_thr)]=0\n",
    "data_df = np.asarray(data_df)\n",
    "\n",
    "#ids=np.random.choice(data_df.shape[0], 221*8)\n",
    "\n",
    "features = np.copy(data_df)\n",
    "labels = np.copy(data_df)\n",
    "\n",
    "print(features.shape[0])\n",
    "print(labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caaf9ee-33e9-4144-ba02-bc5f5ae9db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create dataset object\n",
    "class CustomDataset(TensorDataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        #self.device=device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]#.to(self.device)\n",
    "        data = self.features[idx]#.to(self.device)\n",
    "        sample = {\"Features\": data, \"Labels\": label}\n",
    "        return sample\n",
    "\n",
    "### prepare train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "### use prevalence to rescale:\n",
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "prevalence[prevalence>=0.5]=1\n",
    "prevalence[prevalence<0.5]=0\n",
    "print(1-prevalence.sum()/prevalence.shape[0])\n",
    "X_train_scaled = X_train-prevalence\n",
    "X_test_scaled = X_test-prevalence\n",
    "y_train_scaled = y_train-prevalence\n",
    "y_test_scaled = y_test-prevalence\n",
    "\n",
    "X_train_scaled=torch.from_numpy(X_train_scaled).float()\n",
    "y_train_scaled=torch.from_numpy(y_train_scaled).float()\n",
    "X_test_scaled=torch.from_numpy(X_test_scaled).float()\n",
    "y_test_scaled=torch.from_numpy(y_test_scaled).float()\n",
    "\n",
    "#### Build datasets\n",
    "Train = CustomDataset(X_train_scaled, y_train_scaled)\n",
    "Test = CustomDataset(X_test_scaled, y_test_scaled)\n",
    "\n",
    "### create batch spits of data\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "train_DS = DataLoader(Train, batch_size=100, shuffle=True, drop_last=True, **kwargs)\n",
    "test_DS = DataLoader(Test, batch_size=100, shuffle=True, drop_last=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69b21b9-54db-456c-a153-eb98b71d2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Implement VAE \n",
    "dim=y_train.shape[1]\n",
    "relu = torch.nn.ReLU()\n",
    "input_dim=y_train.shape[1]\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 64)\n",
    "        #self.fc2 = torch.nn.Linear(64, 32)\n",
    "        #self.fc2a = torch.nn.Linear(32, 16)\n",
    "        self.fc3a = torch.nn.Linear(64, 8)\n",
    "        self.fc3b = torch.nn.Linear(64, 8)\n",
    "        self.fc4 = torch.nn.Linear(8, 64)\n",
    "        #self.fc5 = torch.nn.Linear(16, 32)\n",
    "        #self.fc5a = torch.nn.Linear(32, 64)\n",
    "        self.fc6 = torch.nn.Linear(64, input_dim)\n",
    "        \n",
    "        # Define proportion or neurons to dropout\n",
    "        #self.dropout = torch.nn.Dropout(0.2)\n",
    "        '''\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.zeros_(self.fc1.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        torch.nn.init.zeros_(self.fc2.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3a.weight)\n",
    "        torch.nn.init.zeros_(self.fc3a.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc3b.weight)\n",
    "        torch.nn.init.zeros_(self.fc3b.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        torch.nn.init.zeros_(self.fc4.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc5.weight)\n",
    "        torch.nn.init.zeros_(self.fc5.bias)\n",
    "        torch.nn.init.xavier_uniform_(self.fc6.weight)\n",
    "        torch.nn.init.zeros_(self.fc6.bias)\n",
    "       ''' \n",
    "\n",
    "    def encode(self, x):  # 784-400-[20,20]\n",
    "        z = self.fc1(x)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z)\n",
    "        z = torch.tanh(z)\n",
    "        #z = relu(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        #z = self.fc2(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z)\n",
    "        #z = torch.tanh(z)\n",
    "        #z = relu(z)\n",
    "        #z = self.dropout(z)\n",
    "         \n",
    "        z1 = self.fc3a(z)  # u\n",
    "        #z1 = self.fc3a(x)  # u\n",
    "        #z1 = self.dropout(z1)\n",
    "        \n",
    "        z2 = self.fc3b(z)  # logvar\n",
    "        #z2 = self.fc3b(x)  # logvar\n",
    "        #z2 = self.dropout(z2)\n",
    "        return (z1, z2)\n",
    "\n",
    "    def decode(self, z):  # 20-400-784\n",
    "        z = self.fc4(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z)\n",
    "        z = torch.tanh(z)\n",
    "        #z = relu(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        #z = self.fc5(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z)\n",
    "        #z = torch.tanh(z)\n",
    "        #z = relu(z)\n",
    "        #z = self.dropout(z)\n",
    "          \n",
    "        z = self.fc6(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z) ### turn off for abundance prediction\n",
    "        z = torch.tanh(z)\n",
    "        #z = relu(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):  # 784-400-[20,20]-20-400-784\n",
    "        x = x.view(-1, input_dim)\n",
    "        (u, logvar) = self.encode(x)\n",
    "        stdev = torch.exp(0.5 * logvar)\n",
    "        noise = torch.randn_like(stdev)\n",
    "        z = u + 1 * (noise * stdev)  # no noise variation!!!\n",
    "        z = self.decode(z)     # 20-400-784\n",
    "        return (z, u, logvar)\n",
    "\n",
    "### try to penalize many non-zero labels as in L1 lasso?\n",
    "def final_loss(mse_loss, bce_loss, spr_loss, u, logvar, recon_x):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss (BCELoss) and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param bce_loss: recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    MSE = mse_loss\n",
    "    SPR = spr_loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - u.pow(2) - logvar.exp())\n",
    "    #L1 = 0.5 * torch.sum(torch.abs(recon_x))\n",
    "    return KLD + MSE + 0.1 * BCE + 10 * SPR #+ L1 +0.5 * SPR \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fbb3df-4a3a-437b-9c33-0b3561d8493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "model = VAE()#.to(device)\n",
    "\n",
    "# Validation using MSE Loss function\n",
    "criterion_mse = torch.nn.MSELoss(reduction='sum')\n",
    "criterion_bce = torch.nn.BCELoss(reduction='sum')\n",
    "#criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "criterion_kl_loss = torch.nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "# Define your L2 regularization strength for weights\n",
    "l2_lambda = 0.01\n",
    "\n",
    "# Using an Adam Optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr = 1e-3 #,weight_decay = 1e-8\n",
    "                            )\n",
    "### calculate weights by prevalence:\n",
    "prevalence_weights=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "prevalence_weights[prevalence_weights<0.5]=1-prevalence_weights[prevalence_weights<0.5]\n",
    "prevalence_weights=torch.from_numpy(prevalence_weights).float()\n",
    "\n",
    "### iterate through batches\n",
    "epochs = 1000\n",
    "zero_thr = 0.5\n",
    "outputs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_corr = []\n",
    "test_corr = []\n",
    "train_grad = []\n",
    "epochs_without_improvement=0\n",
    "patience=100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #print('Epoch number: ', epoch)\n",
    "    model = model.train()\n",
    "    train_loss = 0.0        # accumulated custom loss\n",
    "    test_loss = 0.0        # accumulated custom loss\n",
    "    corr=0.0\n",
    "    grad_mag=0.0\n",
    "    accs=0.0\n",
    "    for (idx, batch) in enumerate(train_DS):\n",
    "\n",
    "        # Transfer to GPU\n",
    "        #batch['Features'], batch['Labels'] = batch['Features'].to(device), batch['Labels'].to(device)\n",
    "\n",
    "        # Output of VAE\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, u, logvar = model(batch['Features'])\n",
    "\n",
    "        ### calculate MSE loss on a data\n",
    "        mse_loss = criterion_mse(recon_x, batch['Labels'])\n",
    "        ### customized mse_loss\n",
    "        spr_loss = torch.sum(prevalence_weights*torch.sum(torch.square(recon_x-batch['Labels']),dim=0))\n",
    "            \n",
    "        #out = (recon_x.detach()>zero_thr).float()\n",
    "        #bce_loss = criterion_bce(out, batch['Labels'])\n",
    "\n",
    "        #out1 = (recon_x.detach()>zero_thr).float()\n",
    "        #out2 = (recon_x.detach()<-zero_thr).float()\n",
    "        #out=out1-out2\n",
    "        #bce_loss = criterion_bce(torch.abs(out), torch.abs(batch['Labels']))\n",
    "        ### for prev masked\n",
    "        out=((recon_x.detach()+torch.from_numpy(prevalence)[None,:])>zero_thr).float()\n",
    "        #bce_loss = criterion_bce(out, (batch['Labels']+torch.from_numpy(prevalence)[None,:]).float())\n",
    "        bce_loss=0\n",
    "        # Calculating THE loss function\n",
    "        loss_val = final_loss(mse_loss, bce_loss, spr_loss, u, logvar, recon_x)\n",
    "\n",
    "        # Compute L2 regularization loss\n",
    "        l2_reg_loss = 0.0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:\n",
    "                l2_reg_loss += torch.norm(param, 2)\n",
    "\n",
    "        # Add L2 regularization loss to the reconstruction loss\n",
    "        total_loss = loss_val + l2_lambda * l2_reg_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        train_loss += loss_val.item()\n",
    "        \n",
    "        #acc = (recon_x.round() == batch['Labels']).float().mean()\n",
    "        #acc = (out == batch['Labels']).float().mean()\n",
    "        #acc = (out == batch['Features']).float().mean()\n",
    "        ### for prev masked\n",
    "        acc = (out == (batch['Labels']+torch.from_numpy(prevalence)[None,:]).float()).float().mean()\n",
    "        acc = float(acc)\n",
    "        accs +=acc\n",
    "        '''\n",
    "        # Get the magnitude of gradients for each parameter\n",
    "        grad_magnitudes = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_magnitude = torch.norm(param.grad.data)\n",
    "                grad_magnitudes.append(grad_magnitude.item())\n",
    "\n",
    "        # Log the gradient magnitudes for analysis\n",
    "        grad_mag += np.mean(grad_magnitudes)\n",
    "        '''\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Storing the losses in a list for plotting\n",
    "    train_losses.append(train_loss/len(train_DS))\n",
    "    train_acc.append(accs/len(train_DS))\n",
    "    train_grad.append(grad_mag)\n",
    "\n",
    "    accs=0.0\n",
    "    corr=0.0\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for (idx, batch) in enumerate(test_DS):\n",
    "            # Transfer to GPU\n",
    "            #batch['Features'], batch['Labels'] = batch['Features'].to(device), batch['Labels'].to(device)\n",
    "\n",
    "            recon_x, u, logvar = model(batch['Features'])\n",
    "            ### Calculating the loss function\n",
    "            mse_loss = criterion_mse(recon_x, batch['Labels'])\n",
    "            ### customized mse_loss\n",
    "            spr_loss = torch.sum(prevalence_weights*torch.sum(torch.square(recon_x-batch['Labels']),dim=0))\n",
    "            #out = (recon_x.detach()>zero_thr).float()\n",
    "            #bce_loss = criterion_bce(out, batch['Labels'])\n",
    "\n",
    "            ### for prev masked\n",
    "            out=((recon_x.detach()+torch.from_numpy(prevalence)[None,:])>zero_thr).float()\n",
    "            #bce_loss = criterion_bce(out, (batch['Labels']+torch.from_numpy(prevalence)[None,:]).float())\n",
    "            bce_loss=0\n",
    "            loss_val = final_loss(mse_loss, bce_loss, spr_loss, u, logvar, recon_x)\n",
    "            test_loss += loss_val.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            #acc = (out == batch['Features']).float().mean() # for binary-abund\n",
    "            #acc +=np.sum((true_x.shape[1]-np.sum(np.abs(bin_recon_x-true_x), axis=1))/true_x.shape[1])\n",
    "            #acc = (recon_x.round() == batch['Labels']).float().mean()\n",
    "            #acc = (out == batch['Labels']).float().mean()\n",
    "            ### for prev masked\n",
    "            acc = (out == (batch['Labels']+torch.from_numpy(prevalence)[None,:]).float()).float().mean()\n",
    "            acc = float(acc)\n",
    "            accs +=acc\n",
    "\n",
    "        # Storing the losses in a list for plotting\n",
    "        test_losses.append(test_loss/len(test_DS))\n",
    "        test_acc.append(accs/len(test_DS))\n",
    "    \n",
    "    metrics = {'train loss': train_loss/len(train_DS), \n",
    "               'test loss': test_loss/len(test_DS), \n",
    "               'accuracy': accs/len(test_DS),\n",
    "               #'gradient': grad_mag,\n",
    "               #'test_corr': corr/X_test.shape[0]\n",
    "               }\n",
    "    experiment.log_metrics(metrics, step=epoch)\n",
    "    \n",
    "    if train_loss>test_loss:\n",
    "        epochs_without_improvement=0\n",
    "    else:\n",
    "        epochs_without_improvement+=1\n",
    " \n",
    "    if epochs_without_improvement==patience:\n",
    "        print(\"early stop\")\n",
    "        break\n",
    "    #outputs.append((epochs, batch['Features'], reconstructed))\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd85666-c3ee-41d0-b27d-c906ee954ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "#### check how good the reconstruction is by r2:\n",
    "for (idx, batch) in enumerate(test_DS):\n",
    "    print(idx)\n",
    "    true_x = batch['Labels'].detach().numpy()+prevalence\n",
    "    print(true_x)\n",
    "    recon_x, u, logvar = model(batch['Features'])\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    #fig.suptitle('True vs Reconstructed')\n",
    "    \n",
    "    im1=ax1.imshow(true_x[:,range(dim)], cmap='Greys', interpolation='nearest')\n",
    "    ax1.set_title('True')\n",
    "    \n",
    "    a = recon_x.detach().numpy()+prevalence\n",
    "    #a = (recon_x.detach()>zero_thr).float()\n",
    "    a[(a>zero_thr)]=1\n",
    "    a[(a<=zero_thr)]=0\n",
    "    #a=a-prevalence\n",
    "    #ax2.imshow(a[:,range(dim)], cmap='bwr', interpolation='nearest')\n",
    "    im2=ax2.imshow(a[:,range(dim)], cmap='Greys', interpolation='nearest')\n",
    "    ax2.set_title('Reconstructed')\n",
    "    print(1-np.sum(np.abs(true_x-a))/a.shape[0]/a.shape[1])\n",
    "    ax2.figure.colorbar(im2)\n",
    "    #plt.savefig('graphs/batch_reconstr_masked_binary_{}.pdf'.format(idx), dpi=300) \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab80fb4-ba11-4000-ba4c-963e65c3cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc=np.array(train_acc)\n",
    "\n",
    "plt.plot(range(train_acc.shape[0]), train_acc*100, c='blue', label = \"train_acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy, %\")\n",
    "plt.ylim([0, 100])\n",
    "#plt.xlim([0,3000])  # adjust the right leaving left unchanged\n",
    "\n",
    "test_acc=np.array(test_acc)\n",
    "plt.plot(range(test_acc.shape[0]), test_acc*100, c='red', label = \"test_acc\")\n",
    "# horizontal line showing sparsity (average number of zeroes)\n",
    "plt.axhline(y = (1-np.mean(np.sum(X_train, axis=1)/X_train.shape[1]))*100, color = 'grey', linestyle = '-.')\n",
    "# horizontal line showing dumb predictions based on prevalence\n",
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "prevalence[prevalence>=0.5]=1\n",
    "prevalence[prevalence<0.5]=0\n",
    "#plt.axhline(y = (1-prevalence.sum()/prevalence.shape[0])*100, color = 'green', linestyle = '-.')\n",
    "#acc=100-np.mean(np.sum(np.abs(true_x),axis=1))*100//X_train.shape[1]\n",
    "# horizontal line showing dumb predictions based on prevalence\n",
    "#acc=100-np.mean(np.sum(np.abs(true_x+prevalence),axis=1))*100/X_train.shape[1]\n",
    "acc=(1-np.sum(np.abs(X_test-prevalence))/X_test.shape[0]/prevalence.shape[0])*100\n",
    "\n",
    "plt.axhline(y = acc, color = 'orange', linestyle = '-.')\n",
    "\n",
    "train_losses=np.array(train_losses)\n",
    "test_losses=np.array(test_losses)\n",
    "#plt.plot(range(train_acc.shape[0]), train_losses*100/np.max(train_losses), c='blue', label = \"train_loss\", ls='--')\n",
    "#plt.plot(range(train_acc.shape[0]), test_losses*100/np.max(train_losses), c='red', label = \"test_loss\", ls='--')\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "plt.savefig('VAE_masked_wgs_filt_bin_weighted_loss.pdf', dpi=1000) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a07fe-9e08-4bba-bfdc-3ca580eec049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "numba.__version__\n",
    "### let's look at the latent space:\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07b06f6-b7d7-4270-9184-80537f3373e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3000  # Choose an appropriate batch size\n",
    "indices = torch.randperm(len(X_train_scaled))[:batch_size]\n",
    "z=X_train_scaled[indices]\n",
    "# Decode the latent variables to obtain the corresponding data points\n",
    "with torch.no_grad():\n",
    "    encoded, u = model.encode(z)  # Decode latent variables to data points\n",
    "\n",
    "# Convert the decoded data points to numpy array\n",
    "encoded_np = encoded.numpy()\n",
    "\n",
    "# Apply UMAP to reduce the dimensionality of the latent space\n",
    "reducer = umap.UMAP(n_components=2, random_state=0)\n",
    "latent_umap = reducer.fit_transform(encoded_np)\n",
    "\n",
    "# Plot the latent space using a scatter plot\n",
    "plt.scatter(latent_umap[:, 0], latent_umap[:, 1], color ='red', alpha=0.1,)\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('UMAP of points projected to latent space')\n",
    "plt.legend(['Training data'])\n",
    "#plt.savefig('UMAP_of_latent_projections_wgs_filt.pdf', dpi=1200) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5d6f1-e47e-454e-a486-cd35fb10dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3000  # Choose an appropriate batch size\n",
    "\n",
    "# Apply UMAP to reduce the dimensionality of the latent space\n",
    "reducer = umap.UMAP(n_components=2, random_state=0)\n",
    "\n",
    "indices = torch.randperm(len(y_train_scaled))[:batch_size]\n",
    "data_umap = reducer.fit_transform(y_train_scaled[indices])\n",
    "transformed_true=reducer.fit_transform(y_train_scaled[indices])\n",
    "\n",
    "# Plot the latent space using a scatter plot\n",
    "#plt.scatter(data_umap[:, 0], data_umap[:, 1], color ='red', alpha=0.1,)\n",
    "plt.scatter(transformed_true[:, 0], transformed_true[:, 1],  color='red', alpha=0.1)\n",
    "test_umap = reducer.transform(y_test_scaled)\n",
    "plt.scatter(test_umap[:, 0], test_umap[:, 1], color ='blue', alpha=0.1,)\n",
    "\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('UMAP of points sampled from training data')\n",
    "plt.legend(['Training data', 'Test data'])\n",
    "#plt.savefig('UMAP_of_test_data_wgs_filt.pdf', dpi=1200) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb83027-a08a-4b45-b0e2-bd1285dc6944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of data points\n",
    "batch_size = 3000  # Choose an appropriate batch size\n",
    "z_dim = 8  # Choose the dimensionality of the latent space\n",
    "z = torch.randn(batch_size, z_dim)  # Generate random latent variables\n",
    "\n",
    "# Decode the latent variables to obtain the corresponding data points\n",
    "with torch.no_grad():\n",
    "    decoded = model.decode(z)  # Decode latent variables to data points\n",
    "\n",
    "# Convert the decoded data points to numpy array\n",
    "decoded_np = decoded.numpy()\n",
    "\n",
    "# Apply UMAP to reduce the dimensionality of the latent space\n",
    "#reducer = umap.UMAP(n_components=2, random_state=0)\n",
    "#indices = torch.randperm(len(y_train_scaled))[:batch_size]\n",
    "#transformed_true=reducer.fit_transform(y_train_scaled[indices])\n",
    "latent_umap = reducer.transform(decoded_np)\n",
    "\n",
    "# Plot the latent space reconstructions using a scatter plot\n",
    "plt.scatter(transformed_true[:, 0], transformed_true[:, 1],  color='red', alpha=0.1)\n",
    "plt.scatter(latent_umap[:, 0], latent_umap[:, 1], color='blue', alpha=0.1,)\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('UMAP of points sampled from latent space')\n",
    "plt.legend(['Training data','Generated'])\n",
    "#plt.savefig('UMAP_reconstr_train_wgs_filt.pdf', dpi=1200) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1f65a0-3928-42c2-abf7-51e4ec3347a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate true positive:\n",
    "tps=np.zeros((true_x.shape[1], 2))\n",
    "tns=np.zeros((true_x.shape[1], 2))\n",
    "fps=np.zeros((true_x.shape[1], 2))\n",
    "fns=np.zeros((true_x.shape[1], 2))\n",
    "#i=np.zeros((true_x.shape[1], 2))\n",
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "prevalence[prevalence>=0.5]=1\n",
    "prevalence[prevalence<0.5]=0\n",
    "for (idx, batch) in enumerate(test_DS):\n",
    "    zero_thr = 0.0\n",
    "    true_x_bin = batch['Features'].detach().numpy()+prevalence\n",
    "    #true_x_bin= true_x\n",
    "    #true_x_bin[(true_x_bin>zero_thr)]=1\n",
    "    #true_x_bin[(true_x_bin<=zero_thr)]=0\n",
    "    #np.sum(true_x_bin-batch['Features'].detach().numpy())\n",
    "    \n",
    "    ### we need another threshold for reconstructed ones\n",
    "    zero_thr = 0.5\n",
    "    recon_x, u, logvar = model(batch['Features'])\n",
    "    recon_x_bin=recon_x.detach().numpy()+prevalence\n",
    "    recon_x_bin[recon_x_bin>zero_thr]=1\n",
    "    recon_x_bin[recon_x_bin<=zero_thr]=0\n",
    "    print(recon_x_bin.shape)\n",
    "    for i in range(true_x_bin.shape[1]):\n",
    "        tp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        fp=0\n",
    "        #print(true_x_bin.shape[0])\n",
    "        for j in range(true_x_bin.shape[0]):\n",
    "            if(true_x_bin[j,i]==1):\n",
    "                if(recon_x_bin[j,i]==1):\n",
    "                    tp+=1\n",
    "                else:\n",
    "                    fn+=1\n",
    "            else:\n",
    "                if(recon_x_bin[j,i]==1):\n",
    "                    fp+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "        #print(i)\n",
    "        #print(idx)\n",
    "        tps[i, idx]=tp\n",
    "        tns[i, idx]=tn\n",
    "        fps[i, idx]=fp\n",
    "        fns[i, idx]=fn\n",
    "\n",
    "\n",
    "sum_tps=np.sum(tps, axis=1)\n",
    "sum_tns=np.sum(tns, axis=1)\n",
    "sum_fps=np.sum(fps, axis=1)\n",
    "sum_fns=np.sum(fns, axis=1)\n",
    "#print(sum_tps)\n",
    "#sum_tns=tns[:,0]+tns[:,1]\n",
    "#sum_fps=fps[:,0]+fps[:,1]\n",
    "#sum_fns=tps[:,0]+fns[:,1]\n",
    "\n",
    "sensitivity=sum_tps/(sum_tps+sum_fns+0.5)\n",
    "specificity=sum_tns/(sum_tns+sum_fps+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e999ce-bde9-4e06-8064-71869ad39e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_x_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08d440-bbda-41fb-8bf5-5d256eb6fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=sensitivity, y=specificity)\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel(\"Sensitivity\")\n",
    "plt.ylabel(\"Specificity\")\n",
    "plt.legend([\"Spearman r= -0.63\"], frameon=False, loc=3)\n",
    "plt.plot([0.0, 1], [1, 0.0], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.plot([0.2, 1], [1, 0.2], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.4, 1], [1, 0.4], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.6, 1], [1, 0.6], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.8, 1], [1, 0.8], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.savefig('graphs/VAE_wgs_filt_bin_per_species_weighted_loss.pdf', dpi=1000) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb75d8f-6377-4d79-b174-2c90befda0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(sensitivity[sensitivity>0.0], specificity[sensitivity>0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e870f58-512b-49bb-bf95-21156a16db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "plt.scatter(x=prevalence, y=specificity)\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel(\"Prevalence\")\n",
    "plt.ylabel(\"Specificity\")\n",
    "plt.plot([0.0, 1], [1, 0.0], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.2, 1], [1, 0.2], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.legend([\"Spearman r= -0.83\"], frameon=False, loc=3)\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.savefig('VAE_wgs_filt_bin_per_species_prev_spec_weighted_loss.pdf', dpi=1000) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c0cd54-b399-477a-86a3-d0cb72b8ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(prevalence[prevalence<0.5], specificity[prevalence<0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4420e2-b877-45ea-b1c8-68c30d2c51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "plt.scatter(x=prevalence, y=sensitivity)\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel(\"Prevalence\")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.legend([\"Spearman r= 0.86\"], frameon=False, loc=4)\n",
    "plt.plot([0, 1], [0, 1], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "#plt.plot([0, 0.85],[0.43, 1],  ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "#plt.plot([0, 0.6],[0.4, 1],  ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.savefig('VAE_wgs_filt_bin_per_species_prev_sens_weighted_loss.pdf', dpi=1000) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc35532-7fdf-4100-b1e8-ad4ce6672e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(prevalence[sensitivity>0.0], sensitivity[sensitivity>0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88efc715-6fe0-44a5-a16d-2ceb2afb9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### rebalance the data???\n",
    "rare=np.where(prevalence<0.3)[0]\n",
    "X_data=X_train_scaled.numpy()[:,rare]\n",
    "rare_samples=0\n",
    "for i in range(rare.shape[0]):\n",
    "    rare_samples=np.append(rare_samples,np.where(X_data[:,i]>0))\n",
    "    #print(X_train_scaled.numpy()[:,rare[i]])\n",
    "    \n",
    "rare_samples=rare_samples[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544fd0a1-56db-4cc3-a458-c4b3033b53bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(rare_samples).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e6ba77-4304-4283-9e0c-f93bb6a455ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6986c858-5235-4d27-aca1-baa7aea9844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a73b7-2862-43b5-823b-b250bf34b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check sample to sample variability\n",
    "\n",
    "# calculate true positive:\n",
    "tps=np.zeros((true_x.shape[0], 10))\n",
    "tns=np.zeros((true_x.shape[0], 10))\n",
    "fps=np.zeros((true_x.shape[0], 10))\n",
    "fns=np.zeros((true_x.shape[0], 10))\n",
    "#i=np.zeros((true_x.shape[1], 2))\n",
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "prevalence[prevalence>=0.5]=1\n",
    "prevalence[prevalence<0.5]=0\n",
    "for (idx, batch) in enumerate(train_DS):\n",
    "    zero_thr = 0.0\n",
    "    true_x_bin = batch['Features'].detach().numpy()+prevalence\n",
    "    #true_x_bin= true_x\n",
    "    #true_x_bin[(true_x_bin>zero_thr)]=1\n",
    "    #true_x_bin[(true_x_bin<=zero_thr)]=0\n",
    "    #np.sum(true_x_bin-batch['Features'].detach().numpy())\n",
    "    \n",
    "    ### we need another threshold for reconstructed ones\n",
    "    zero_thr = 0.5\n",
    "    recon_x, u, logvar = model(batch['Features'])\n",
    "    recon_x_bin=recon_x.detach().numpy()+prevalence\n",
    "    recon_x_bin[recon_x_bin>zero_thr]=1\n",
    "    recon_x_bin[recon_x_bin<=zero_thr]=0\n",
    "    print(recon_x_bin.shape)\n",
    "    for i in range(true_x_bin.shape[0]):\n",
    "        tp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        fp=0\n",
    "        #print(true_x_bin.shape[0])\n",
    "        for j in range(true_x_bin.shape[1]):\n",
    "            if(true_x_bin[i,j]==1):\n",
    "                if(recon_x_bin[i,j]==1):\n",
    "                    tp+=1\n",
    "                else:\n",
    "                    fn+=1\n",
    "            else:\n",
    "                if(recon_x_bin[i,j]==1):\n",
    "                    fp+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "        #print(i)\n",
    "        #print(idx)\n",
    "        tps[i, idx]=tp\n",
    "        tns[i, idx]=tn\n",
    "        fps[i, idx]=fp\n",
    "        fns[i, idx]=fn\n",
    "\n",
    "\n",
    "sum_tps=tps.flatten()\n",
    "sum_tns=tns.flatten()\n",
    "sum_fps=fps.flatten()\n",
    "sum_fns=fns.flatten()\n",
    "#print(sum_tps)\n",
    "#sum_tns=tns[:,0]+tns[:,1]\n",
    "#sum_fps=fps[:,0]+fps[:,1]\n",
    "#sum_fns=tps[:,0]+fns[:,1]\n",
    "\n",
    "sensitivity=sum_tps/(sum_tps+sum_fns)#+0.5)\n",
    "specificity=sum_tns/(sum_tns+sum_fps)#+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3d1ff-9564-4ab5-a327-d918666374aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd465c7c-3a27-4aa5-b69b-6ea19d1f9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=sensitivity, y=specificity, alpha=0.3)\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel(\"Sensitivity\")\n",
    "plt.ylabel(\"Specificity\")\n",
    "plt.legend([\"Spearman r= -0.22\"], frameon=False, loc=3)\n",
    "plt.plot([0.0, 1], [1, 0.0], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.plot([0.2, 1], [1, 0.2], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.4, 1], [1, 0.4], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.6, 1], [1, 0.6], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.8, 1], [1, 0.8], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.savefig('graphs/VAE_wgs_filt_bin_per_samples_train_weighted_loss.pdf', dpi=1000) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852ecbf-0d59-44f9-9518-30a0ad0e9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(sensitivity, specificity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3b62e-1fda-4519-879d-2807da42d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Renormalize data to sum=1 (enforce it?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae976e-86e5-41c2-88c3-1595e211e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO: NN baseline for accuracy (reuse code from SNP-genes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b721c828-ad85-43e7-949c-0fc575c5577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO: PCA->embed->reverse PCA (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08fbd28-553d-42d4-8c4d-a6a4056f231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Categorize abundance for accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100c6b52-d74a-40d2-b7fb-45d6162d4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reweight loss function with penalty proportional to the reverse distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6345b2-046d-4690-8817-24db7a813006",
   "metadata": {},
   "outputs": [],
   "source": [
    "### try other data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad216f6-c6fe-4d4a-bf95-4b78050f3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### try to memorize perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6ce45b-3a6b-455c-8c0f-6670820d4d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract what model learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9011e0a8-e7fd-4597-bcca-4057ec64cfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### code refactoring"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
