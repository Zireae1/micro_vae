{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905b8af-022e-434c-b934-c2fa2ae3f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import comet_ml at the top of your file\n",
    "# from comet_ml import Experiment  ### special library to record performance on a web server\n",
    "# from config import * ### file with personal API_KEY\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from scipy import stats\n",
    "from skbio.stats.composition import clr\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:1' if use_cuda else 'mps')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a4b25-0fb2-4672-bbda-f5be2b01ca13",
   "metadata": {},
   "source": [
    "### Record experiment on comet.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9f4bd-b469-45d8-9241-70ba8f1f9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize comet experiment with your api key\n",
    "'''\n",
    "experiment = Experiment(\n",
    "    api_key=API_KEY,\n",
    "    project_name=\"vae-eco-ml\",\n",
    "    workspace=\"zireae1\",\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5d617-e9e8-495a-9156-f08a998597e3",
   "metadata": {},
   "source": [
    "### Load clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1435d-eadd-4b4e-8f03-466b914f1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "### data: rows are samples, columns are species, relative abundance sum ~ [80-100]\n",
    "\n",
    "data_df = pd.read_csv(\"data/wgs_train_health.noab_data_to_ml.filt.txt\", sep=\"\\t\")\n",
    "data_df = data_df.to_numpy()\n",
    "print(data_df.shape)\n",
    "\n",
    "zero_thr = 1e-6\n",
    "features = np.copy(data_df)\n",
    "features[(features>zero_thr)]=1 # binarize only features\n",
    "features[(features<=zero_thr)]=0\n",
    "labels = np.copy(data_df)\n",
    "\n",
    "#features = np.asarray(features)#/100\n",
    "print(features.shape[0])\n",
    "#labels = np.asarray(labels)#/100\n",
    "print(labels.shape[0])\n",
    "### check that features are binary:\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bdaa3d-1478-4855-9ddf-7dab9598c07b",
   "metadata": {},
   "source": [
    "### Create dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcaf693-568e-438a-9c60-19fe174b687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(TensorDataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.labels = labels\n",
    "        self.features = features\n",
    "        #self.device=device\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]#.to(self.device)\n",
    "        data = self.features[idx]#.to(self.device)\n",
    "        sample = {\"Features\": data, \"Labels\": label}\n",
    "        return sample\n",
    "\n",
    "### prepare train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "### clr transformation for outputs\n",
    "gmean_train = (np.exp(np.nansum(np.log(y_train[y_train > 0]+zero_thr)) / np.size(y_train)))\n",
    "y_train_clr = np.log((y_train+zero_thr)/gmean_train)\n",
    "y_test_clr = np.log((y_test+zero_thr)/gmean_train)\n",
    "\n",
    "### rescale to [-1, 1] interval\n",
    "scaler = preprocessing.MaxAbsScaler().fit(y_train_clr)\n",
    "#scaler = preprocessing.MinMaxScaler().fit(y_train_scaled)\n",
    "y_train_scaled = scaler.transform(y_train_clr)\n",
    "y_test_scaled = scaler.transform(y_test_clr)\n",
    "\n",
    "### transform to tensors\n",
    "X_train_scaled=torch.from_numpy(X_train).float()\n",
    "y_train_scaled=torch.from_numpy(y_train_scaled).float()\n",
    "X_test_scaled=torch.from_numpy(X_test).float()\n",
    "y_test_scaled=torch.from_numpy(y_test_scaled).float()\n",
    "\n",
    "#X_train=X_train.to(device)\n",
    "#y_train=y_train.to(device)\n",
    "#X_test=X_test.to(device)\n",
    "#y_test=y_test.to(device)\n",
    "\n",
    "#### dataset build \n",
    "Train = CustomDataset(X_train_scaled, y_train_scaled)\n",
    "Test = CustomDataset(X_test_scaled, y_test_scaled)\n",
    "\n",
    "### create batch spits of data\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "train_DS = DataLoader(Train, batch_size=100, shuffle=True, drop_last=True, **kwargs)\n",
    "test_DS = DataLoader(Test, batch_size=100, shuffle=True, drop_last=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67df3cf2-40ff-476b-8756-da8578a47d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check distribution of the data\n",
    "plt.hist(y_test_scaled.flatten(), bins=40)\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "#plt.savefig('wgs_filt.pdf', dpi=1000) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2d1fbd-55eb-47af-b1eb-9dfd6af3a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate mean for training to compute residual loss:\n",
    "#train_means=np.mean(y_train_scaled.numpy(), axis=0)\n",
    "#print(np.mean(train_means))\n",
    "\n",
    "### more sophisticated way (mean across samples where species was present):\n",
    "train_means=(torch.sum(X_train_scaled*y_train_scaled, axis=0)/torch.sum(X_train_scaled, axis=0)).numpy()\n",
    "train_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0443d-f029-4dcf-8535-f63c6a769140",
   "metadata": {},
   "source": [
    "### Implement VAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014eaff9-bca6-434e-8c2d-bd14352c2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "#relu = torch.nn.ELU()\n",
    "input_dim=y_train.shape[1]\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, 1024)\n",
    "        #self.fc2 = torch.nn.Linear(90, 80)\n",
    "        self.fc3a = torch.nn.Linear(1024, 64)\n",
    "        self.fc3b = torch.nn.Linear(1024, 64)\n",
    "        self.fc4 = torch.nn.Linear(64, 1024)\n",
    "        #self.fc5 = torch.nn.Linear(26, 52)\n",
    "        self.fc6 = torch.nn.Linear(1024, input_dim)\n",
    "        \n",
    "        # Define proportion or neurons to dropout\n",
    "        self.dropout = torch.nn.Dropout(0.1)  \n",
    "\n",
    "    def encode(self, x):  # 784-400-[20,20]\n",
    "        z = self.fc1(x)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z)\n",
    "        #z = torch.tanh(z)\n",
    "        z = relu(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        #z = self.fc2(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z)\n",
    "        #z = torch.tanh(z)\n",
    "        #z = relu(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        z1 = self.fc3a(z)  # u\n",
    "        #z1 = self.fc3a(x)  # u\n",
    "        #z1 = self.dropout(z1)\n",
    "        \n",
    "        z2 = self.fc3b(z)  # logvar\n",
    "        #z2 = self.fc3b(x)  # logvar\n",
    "        #z2 = self.dropout(z2)\n",
    "        return (z1, z2)\n",
    "\n",
    "    def decode(self, z):  # 20-400-784\n",
    "        z = self.fc4(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z)\n",
    "        #z = torch.tanh(z)\n",
    "        z = relu(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        #z = self.fc5(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z)\n",
    "        #z = torch.tanh(z)\n",
    "        #z = relu(z)\n",
    "        #z = self.dropout(z)\n",
    "        \n",
    "        z = self.fc6(z)\n",
    "        z = self.dropout(z)\n",
    "        \n",
    "        #z = torch.sigmoid(z) ### turn off for abundance prediction\n",
    "        z = torch.tanh(z)\n",
    "        #z = relu(z)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):  # \n",
    "        x = x.view(-1, input_dim)\n",
    "        (u, logvar) = self.encode(x)\n",
    "        stdev = torch.exp(0.5 * logvar)\n",
    "        noise = torch.randn_like(stdev)\n",
    "        z = u + 1 * (noise * stdev)  \n",
    "        z = self.decode(z)    \n",
    "        return (z, u, logvar)\n",
    "\n",
    "### try to penalize many non-zero labels as in L1 lasso? [did not help]\n",
    "def final_loss(mse_loss, bce_loss, spr_loss, u, logvar, recon_x):\n",
    "    \"\"\"\n",
    "    This function will add the reconstruction loss and the \n",
    "    KL-Divergence.\n",
    "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    :param mse_loss: mse recontruction loss\n",
    "    :param bce_loss: bce recontruction loss\n",
    "    :param spr_loss: custom term in recontruction loss\n",
    "    :param mu: the mean from the latent vector\n",
    "    :param logvar: log variance from the latent vector\n",
    "    \"\"\"\n",
    "    BCE = bce_loss \n",
    "    MSE = mse_loss\n",
    "    SPR = spr_loss\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - u.pow(2) - logvar.exp())\n",
    "    return 0.01 * KLD + MSE + 10 * SPR + 1 * BCE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f6928f-9eec-4367-8053-896b65a01ee6",
   "metadata": {},
   "source": [
    "### Model Initialization and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57772e4-a0d6-4ab3-8611-491ca2d4ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE()#.to(device)\n",
    "\n",
    "### Loss functions:\n",
    "criterion_mse = torch.nn.MSELoss(reduction='sum')\n",
    "#criterion_mse = RMSELoss()\n",
    "criterion_bce = torch.nn.BCELoss(reduction='sum')\n",
    "#criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "criterion_l1 = torch.nn.L1Loss()\n",
    "criterion_kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "\n",
    "### Define your L2 regularization strength for model weights\n",
    "l2_lambda = 0.01\n",
    "\n",
    "### Using an Adam Optimizer \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3 ,weight_decay = 1e-7)\n",
    "\n",
    "'''\n",
    "### calculate weights by prevalence (if needed):\n",
    "prevalence_weights=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "prevalence_weights[prevalence_weights<0.5]=1-prevalence_weights[prevalence_weights<0.5]\n",
    "prevalence_weights=torch.from_numpy(prevalence_weights).float()\n",
    "'''\n",
    "\n",
    "### iterate through batches\n",
    "epochs = 1000\n",
    "zero_thr = -0.6 ### picked based on the distribution above\n",
    "outputs = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train_corr = []\n",
    "test_corr = []\n",
    "train_grad = []\n",
    "epochs_without_improvement=0\n",
    "patience=100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #print('Epoch number: ', epoch)\n",
    "    model = model.train()\n",
    "    train_loss = 0.0        # accumulated custom loss\n",
    "    test_loss = 0.0        # accumulated custom loss\n",
    "    corr=0.0\n",
    "    grad_mag=0.0\n",
    "    accs=0.0\n",
    "    \n",
    "    ### VAE training\n",
    "    for (idx, batch) in enumerate(train_DS):\n",
    "\n",
    "        ### Transfer to GPU\n",
    "        #batch['Features'], batch['Labels'] = batch['Features'].to(device), batch['Labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, u, logvar = model(batch['Features'])\n",
    "\n",
    "        ### calculate MSE loss on a data\n",
    "        #mse_loss = torch.sqrt(criterion_mse(recon_x, batch['Labels']))\n",
    "        mse_loss = criterion_mse(recon_x, batch['Labels'])\n",
    "        \n",
    "        spr_loss = criterion_mse(recon_x-torch.from_numpy(train_means)[None,:], batch['Labels']-torch.from_numpy(train_means)[None,:])\n",
    "        #spr_loss=0\n",
    "\n",
    "        out = ((recon_x.detach())>zero_thr).float() ### for MaxAbs scaled\n",
    "        \n",
    "        ### try penalize only non-zero zeroes\n",
    "        #bce_loss = torch.sum(torch.abs((1-batch['Features'])*(recon_x-batch['Labels'])))\n",
    "        \n",
    "        ### try to penalize more according to the entropy of species??? same as prevalence:\n",
    "        #entropy2 = Categorical(probs = torch.transpose(batch['Features'],0, 1)).entropy()\n",
    "        \n",
    "        #bce_loss = criterion_bce(torch.abs(out), torch.abs(batch['Labels']))\n",
    "        bce_loss=0 ### addition of BCE loss did not help\n",
    "        \n",
    "        ### for prev masked\n",
    "        #out=((recon_x.detach()+torch.from_numpy(prevalence)[None,:])>zero_thr).float()\n",
    "        #bce_loss = criterion_bce(out, (batch['Features']+torch.from_numpy(prevalence)[None,:]).float())\n",
    "\n",
    "        ### Calculating THE loss function\n",
    "        loss_val = final_loss(mse_loss, bce_loss, spr_loss, u, logvar, recon_x)\n",
    "        \n",
    "        ### Compute L2 regularization loss\n",
    "        l2_reg_loss = 0.0\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:\n",
    "                l2_reg_loss += torch.norm(param, 2)\n",
    "        \n",
    "        ### Add L2 regularization loss to the reconstruction loss\n",
    "        total_loss = loss_val + l2_lambda * l2_reg_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        train_loss += loss_val.item()\n",
    "        \n",
    "        acc = (out == batch['Features']).float().mean()\n",
    "        acc = float(acc)\n",
    "        accs +=acc\n",
    "        \n",
    "        '''\n",
    "        # Get the magnitude of gradients for each parameter\n",
    "        grad_magnitudes = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_magnitude = torch.norm(param.grad.data)\n",
    "                grad_magnitudes.append(grad_magnitude.item())\n",
    "\n",
    "        # Log the gradient magnitudes for analysis\n",
    "        grad_mag += np.mean(grad_magnitudes)\n",
    "        '''\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Storing the losses in a list for plotting\n",
    "    train_losses.append(train_loss/len(train_DS))\n",
    "    train_acc.append(accs/len(train_DS))\n",
    "    train_grad.append(grad_mag)\n",
    "\n",
    "    ### VAE evaluation\n",
    "    accs=0.0\n",
    "    corr=0.0\n",
    "    with torch.no_grad():\n",
    "        model = model.eval()\n",
    "        for (idx, batch) in enumerate(test_DS):\n",
    "            ### Transfer to GPU\n",
    "            #batch['Features'], batch['Labels'] = batch['Features'].to(device), batch['Labels'].to(device)\n",
    "\n",
    "            recon_x, u, logvar = model(batch['Features'])\n",
    "            ### Calculating the loss function\n",
    "            mse_loss = criterion_mse(recon_x, batch['Labels'])\n",
    "            #mse_loss = 0\n",
    "\n",
    "            out = ((recon_x.detach())>zero_thr).float() #for MaxAbs scaled\n",
    "            #bce_loss = criterion_bce(out, batch['Labels'])\n",
    "            bce_loss=0\n",
    "            \n",
    "            ### try penalize only non-zero zeroes\n",
    "            #bce_loss = torch.sum(torch.abs((1-batch['Features'])*(recon_x-batch['Labels'])))\n",
    "            \n",
    "            spr_loss = criterion_mse(recon_x-torch.from_numpy(train_means)[None,:], batch['Labels']-torch.from_numpy(train_means)[None,:])\n",
    "            #spr_loss=0\n",
    "            \n",
    "            loss_val = final_loss(mse_loss, bce_loss, spr_loss, u, logvar, recon_x)\n",
    "            test_loss += loss_val.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            acc = (out == batch['Features']).float().mean() # for binary-abund\n",
    "            \n",
    "            #acc = (out == (batch['Features']+torch.from_numpy(prevalence)[None,:]).float()).float().mean()\n",
    "            acc = float(acc)\n",
    "            accs +=acc\n",
    "\n",
    "        # Storing the losses in a list for plotting\n",
    "        test_losses.append(test_loss/len(test_DS))\n",
    "        test_acc.append(accs/len(test_DS))\n",
    "        \n",
    "    ### Store metrics for comet:\n",
    "    '''\n",
    "    metrics = {'train loss': train_loss/len(train_DS), \n",
    "               'test loss': test_loss/len(test_DS), \n",
    "               'accuracy': accs/len(test_DS),\n",
    "               #'gradient': grad_mag,\n",
    "               #'test_corr': corr/X_test.shape[0]\n",
    "               }\n",
    "    experiment.log_metrics(metrics, step=epoch)\n",
    "    '''\n",
    "    if train_loss>test_loss:\n",
    "        epochs_without_improvement=0\n",
    "    else:\n",
    "        epochs_without_improvement+=1\n",
    " \n",
    "    if epochs_without_improvement==patience:\n",
    "        print(\"early stop\")\n",
    "        break\n",
    "    #outputs.append((epochs, batch['Features'], reconstructed))\n",
    "    \n",
    "# experiment.end() ### end comet experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46d538-a2ee-48b0-b15b-b1f8c5951055",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check the distribution of reconstructed data:\n",
    "plt.hist(recon_x.detach().numpy().flatten(), bins=40)\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "#plt.savefig('wgs_filt_clr_mse_loss_wide_v2.pdf', dpi=1000) \n",
    "plt.show()\n",
    "#plt.hist(recon_x.detach().numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21871e0-71ec-4012-b587-aec5e9809b5e",
   "metadata": {},
   "source": [
    "### Look at test set reconstruction quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dc3934-dd90-4a09-87bd-9fe354b1b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### check how good the reconstructions are (plot matrices as heatmaps):\n",
    "for (idx, batch) in enumerate(test_DS):\n",
    "    print(idx)\n",
    "    true_x = batch['Labels'].detach().numpy()\n",
    "    #print(true_x)\n",
    "    recon_x, u, logvar = model(batch['Features'])\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    #fig.suptitle('True vs Reconstructed')\n",
    "    \n",
    "    ax1.imshow(true_x[:,range(input_dim)], cmap='bwr', interpolation='nearest')\n",
    "    ax1.set_title('True')\n",
    "    \n",
    "    a = recon_x.detach().numpy()\n",
    "\n",
    "    ax2.imshow(a[:,range(input_dim)], cmap='bwr', interpolation='nearest')\n",
    "    ax2.set_title('Reconstructed')\n",
    "    #print(1-np.sum(np.abs(true_x)-np.abs(a))/a.shape[0]/a.shape[1])\n",
    "    print(stats.spearmanr(true_x.flatten(), a.flatten()))\n",
    "    #plt.savefig('batch_reconstruction_masked_v1_{}_wide.pdf'.format(idx), dpi=1300) \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520327c-6661-4d27-a660-559741ed47f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### check how good the reconstructions are (plot predicted vs observed scatter plot):\n",
    "\n",
    "for (idx, batch) in enumerate(test_DS):\n",
    "    print(idx)\n",
    "    true_x = batch['Labels'].detach().numpy()\n",
    "    #print(true_x)\n",
    "    recon_x, u, logvar = model(batch['Features'])\n",
    "    \n",
    "    a = recon_x.detach().numpy()\n",
    "    #fig.suptitle('True vs Reconstructed')\n",
    "    #plt.scatter(x=true_x, y=a)\n",
    "    plt.hist2d(x=true_x.flatten(), y=a.flatten(), norm=matplotlib.colors.PowerNorm(1/10), bins=100)\n",
    "\n",
    "    #ax1.imshow(true_x[:,range(input_dim)], cmap='bwr', interpolation='nearest')\n",
    "    #ax1.set_title('True')\n",
    "    \n",
    "    \n",
    "\n",
    "    #ax2.imshow(a[:,range(input_dim)], cmap='bwr', interpolation='nearest')\n",
    "    #ax2.set_title('Reconstructed')\n",
    "    \n",
    "    print(stats.spearmanr(true_x.flatten(), a.flatten()))\n",
    "    #plt.savefig('batch_reconstruction_masked_v1_{}_wide.pdf'.format(idx), dpi=1300) \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5051750-3d7c-4cb4-9841-17d304e729ce",
   "metadata": {},
   "source": [
    "### Plot model training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960669f8-071d-4069-81b5-51a79121fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc=np.array(train_acc)\n",
    "\n",
    "plt.plot(range(train_acc.shape[0]), train_acc*100, c='blue', label = \"train_acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy, %\")\n",
    "plt.ylim([0, 100])\n",
    "plt.xlim([0,1000])  # adjust the right leaving left unchanged\n",
    "\n",
    "test_acc=np.array(test_acc)\n",
    "plt.plot(range(test_acc.shape[0]), test_acc*100, c='red', label = \"test_acc\")\n",
    "# horizontal line showing sparsity (average number of zeroes)\n",
    "#plt.axhline(y = (1-np.mean(np.sum(X_train+1, axis=1)/X_train.shape[1]))*100, color = 'grey', linestyle = '-.')\n",
    "# horizontal line showing dumb predictions based on prevalence\n",
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "prevalence[prevalence>=0.5]=1\n",
    "prevalence[prevalence<0.5]=0\n",
    "#plt.axhline(y = (1-prevalence.sum()/prevalence.shape[0])*100, color = 'green', linestyle = '-.')\n",
    "#acc=(1-np.mean(np.sum(y_train_scaled.numpy()+1, axis=1)/y_train_scaled.shape[1]))*100\n",
    "# horizontal line showing dumb predictions based on prevalence\n",
    "#acc=100-np.mean(np.sum(np.abs(true_x+prevalence),axis=1))*100/X_train.shape[1]\n",
    "acc=(1-np.sum(np.abs(X_test_scaled.numpy()-prevalence))/X_test_scaled.shape[0]/prevalence.shape[0])*100\n",
    "#acc=1-np.mean(np.sum(X_test_scaled.numpy()+1-prevalence))\n",
    "plt.axhline(y = acc, color = 'orange', linestyle = '-.')\n",
    "\n",
    "train_losses=np.array(train_losses)\n",
    "test_losses=np.array(test_losses)\n",
    "plt.plot(range(train_acc.shape[0]), train_losses*100/np.max(train_losses), c='blue', label = \"train_loss\", ls='--')\n",
    "plt.plot(range(train_acc.shape[0]), test_losses*100/np.max(train_losses), c='red', label = \"test_loss\", ls='--')\n",
    "\n",
    "plt.legend(frameon=False)\n",
    "#plt.savefig('VAE_masked_wgs_filt_nonb_lat8_wide.pdf', dpi=1000) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48aed5-0c27-4108-b328-a8be72c2abee",
   "metadata": {},
   "source": [
    "### Count true/false positive/negative _per species_ statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8737b9-6164-4bc3-a2c3-11249302a8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps=np.zeros((true_x.shape[1], len(test_DS)))\n",
    "tns=np.zeros((true_x.shape[1], len(test_DS)))\n",
    "fps=np.zeros((true_x.shape[1], len(test_DS)))\n",
    "fns=np.zeros((true_x.shape[1], len(test_DS)))\n",
    "\n",
    "zero_thr = -0.6\n",
    "for (idx, batch) in enumerate(test_DS):\n",
    "    print(\"num\")\n",
    "    true_x_bin = batch['Features'].detach().numpy()\n",
    "  \n",
    "    recon_x, u, logvar = model(batch['Features'])\n",
    "    recon_x_bin=recon_x.detach().numpy()#+1\n",
    "    recon_x_bin[recon_x_bin>zero_thr]=1\n",
    "    recon_x_bin[recon_x_bin<=zero_thr]=0\n",
    "    print(recon_x_bin.shape)\n",
    "    for i in range(true_x_bin.shape[1]):\n",
    "        tp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        fp=0\n",
    "        #print(true_x_bin.shape[0])\n",
    "        for j in range(true_x_bin.shape[0]):\n",
    "            if(true_x_bin[j,i]==1):\n",
    "                if(recon_x_bin[j,i]==1):\n",
    "                    tp+=1\n",
    "                else:\n",
    "                    fn+=1\n",
    "            else:\n",
    "                if(recon_x_bin[j,i]==1):\n",
    "                    fp+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "        #print(i)\n",
    "        tps[i, idx]=tp\n",
    "        tns[i, idx]=tn\n",
    "        fps[i, idx]=fp\n",
    "        fns[i, idx]=fn\n",
    "\n",
    "### Summarize across batches\n",
    "sum_tps=np.sum(tps, axis=1)\n",
    "sum_tns=np.sum(tns, axis=1)\n",
    "sum_fps=np.sum(fps, axis=1)\n",
    "sum_fns=np.sum(fns, axis=1)\n",
    "\n",
    "sensitivity=sum_tps/(sum_tps+sum_fns)#+0.5)\n",
    "specificity=sum_tns/(sum_tns+sum_fps)#+0.5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa41fc-63ad-4f88-b4b3-b34deff4be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_DS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efba2229-cf5d-4bfc-b806-dd88f4ed222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=sensitivity, y=specificity)\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel(\"Sensitivity\")\n",
    "plt.ylabel(\"Specificity\")\n",
    "plt.legend([\"Spearman r= -0.01\"], frameon=False, loc=3)\n",
    "plt.plot([0.0, 1], [1, 0.0], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.plot([0.2, 1], [1, 0.2], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.4, 1], [1, 0.4], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.legend([\"Spearman r=\" + str(np.round(stats.spearmanr(sensitivity, specificity)[0], decimals=3))], frameon=False, loc=3)\n",
    "\n",
    "#plt.savefig('VAE_wgs_filt_nonb_per_species_metrics_residual_loss_wide.pdf', dpi=1000) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247916d-c7d0-428c-a276-a5804008518f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "plt.scatter(x=prevalence, y=specificity)\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel(\"Prevalence\")\n",
    "plt.ylabel(\"Specificity\")\n",
    "plt.plot([0.0, 1], [1, 0.0], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.legend([\"Spearman r= -0.54\"], frameon=False, loc=3)\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.legend([\"Spearman r=\" + str(np.round(stats.spearmanr(prevalence, specificity)[0], decimals=3))], frameon=False, loc=3)\n",
    "\n",
    "#plt.savefig('VAE_wgs_filt_nonb_per_species_prev_residual_loss_wide.pdf', dpi=1000) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f4fc7-55e2-4683-afc1-08aa52dfe81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "plt.scatter(x=prevalence, y=sensitivity)\n",
    "\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.xlabel(\"Prevalence\")\n",
    "plt.ylabel(\"Sensitivity\")\n",
    "plt.legend([\"Spearman r= 0.73\"], frameon=False, loc=4)\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.legend([\"Spearman r=\" + str(np.round(stats.spearmanr(prevalence, sensitivity)[0], decimals=3))], frameon=False, loc=3)\n",
    "\n",
    "#plt.savefig('VAE_wgs_filt_nonb_per_species_prev_sens_residual_loss_wide.pdf', dpi=1000) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903ae22f-c71f-48bd-8d61-332df0f64593",
   "metadata": {},
   "source": [
    "### Count true/false positive/negative _per sample_ statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee9c7f-7ddb-4c00-b092-5864565c5243",
   "metadata": {},
   "outputs": [],
   "source": [
    "tps=np.zeros((true_x.shape[0], len(test_DS)))\n",
    "tns=np.zeros((true_x.shape[0], len(test_DS)))\n",
    "fps=np.zeros((true_x.shape[0], len(test_DS)))\n",
    "fns=np.zeros((true_x.shape[0], len(test_DS)))\n",
    "#i=np.zeros((true_x.shape[1], 2))\n",
    "prevalence=np.sum(X_train, axis=0)/X_train.shape[0]\n",
    "prevalence[prevalence>=0.5]=1\n",
    "prevalence[prevalence<0.5]=0\n",
    "zero_thr = -0.6\n",
    "for (idx, batch) in enumerate(test_DS):\n",
    "    true_x_bin = batch['Features'].detach().numpy()#+prevalence\n",
    "  \n",
    "    recon_x, u, logvar = model(batch['Features'])\n",
    "    recon_x_bin=recon_x.detach().numpy()\n",
    "    recon_x_bin[recon_x_bin>zero_thr]=1\n",
    "    recon_x_bin[recon_x_bin<=zero_thr]=0\n",
    "    print(recon_x_bin.shape)\n",
    "    for i in range(true_x_bin.shape[0]):\n",
    "        tp=0\n",
    "        tn=0\n",
    "        fn=0\n",
    "        fp=0\n",
    "        #print(true_x_bin.shape[0])\n",
    "        for j in range(true_x_bin.shape[1]):\n",
    "            if(true_x_bin[i,j]==1):\n",
    "                if(recon_x_bin[i,j]==1):\n",
    "                    tp+=1\n",
    "                else:\n",
    "                    fn+=1\n",
    "            else:\n",
    "                if(recon_x_bin[i,j]==1):\n",
    "                    fp+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "        #print(i)\n",
    "        tps[i, idx]=tp\n",
    "        tns[i, idx]=tn\n",
    "        fps[i, idx]=fp\n",
    "        fns[i, idx]=fn\n",
    "\n",
    "### Summarize across batches\n",
    "sum_tps=tps.flatten()\n",
    "sum_tns=tns.flatten()\n",
    "sum_fps=fps.flatten()\n",
    "sum_fns=fns.flatten()\n",
    "\n",
    "sensitivity=sum_tps/((sum_tps+sum_fns))#+0.5)\n",
    "specificity=sum_tns/((sum_tns+sum_fps))#+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f412d3-b987-40f9-b5d3-195ac9cda541",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=sensitivity, y=specificity, alpha=0.3)\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xlim([0, 1.1])\n",
    "plt.xlabel(\"Sensitivity\")\n",
    "plt.ylabel(\"Specificity\")\n",
    "plt.legend([\"Spearman r= -0.98\"], frameon=False, loc=3)\n",
    "plt.plot([0.0, 1], [1, 0.0], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.plot([0.2, 1], [1, 0.2], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.4, 1], [1, 0.4], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.6, 1], [1, 0.6], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "plt.plot([0.8, 1], [1, 0.8], ls=\"--\", linewidth=\".3\", c=\".3\")\n",
    "\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.legend([\"Spearman r=\" + str(np.round(stats.spearmanr(sensitivity, specificity)[0], decimals=3))], frameon=False, loc=3)\n",
    "\n",
    "#plt.savefig('graphs/VAE_wgs_filt_nonbin_per_samples_train_weighted_loss_wide.pdf', dpi=1000) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6ead6-3663-495d-adab-9dea6d85ac84",
   "metadata": {},
   "source": [
    "### Let's look at the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b63f1-cca4-4885-aa8c-e13f66f84c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "numba.__version__\n",
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e7d23d-dbcd-42ad-ab85-471269ddbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000  # Choose an appropriate batch size\n",
    "indices = torch.randperm(len(X_train_scaled))[:batch_size]\n",
    "z=X_train_scaled[indices]\n",
    "\n",
    "# Decode the latent variables to obtain the corresponding data points\n",
    "with torch.no_grad():\n",
    "    encoded, u = model.encode(z)  # Decode latent variables to data points\n",
    "\n",
    "# Convert the decoded data points to numpy array\n",
    "encoded_np = encoded.numpy()\n",
    "\n",
    "# Apply UMAP to reduce the dimensionality of the latent space\n",
    "reducer = umap.UMAP(n_components=2, random_state=0)\n",
    "latent_umap = reducer.fit_transform(encoded_np)\n",
    "\n",
    "# Plot the latent space using a scatter plot\n",
    "plt.scatter(latent_umap[:, 0], latent_umap[:, 1], color ='red', alpha=0.1,)\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('UMAP of points projected to latent space')\n",
    "plt.legend(['Training data'])\n",
    "#plt.savefig('UMAP_of_latent_projections_wgs_filt.pdf', dpi=1200) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457e4a38-b354-423d-b333-47bd91ee55e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000  # Choose an appropriate batch size\n",
    "\n",
    "indices = torch.randperm(len(y_train_scaled))[:batch_size]\n",
    "data_umap = reducer.fit_transform(y_train_scaled[indices])\n",
    "transformed_true=reducer.fit_transform(y_train_scaled[indices])\n",
    "\n",
    "# Plot the latent space using a scatter plot\n",
    "#plt.scatter(data_umap[:, 0], data_umap[:, 1], color ='red', alpha=0.1,)\n",
    "plt.scatter(transformed_true[:, 0], transformed_true[:, 1],  color='red', alpha=0.1)\n",
    "test_umap = reducer.transform(y_test_scaled)\n",
    "plt.scatter(test_umap[:, 0], test_umap[:, 1], color ='blue', alpha=0.1,)\n",
    "\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('UMAP of points sampled from training and test data')\n",
    "plt.legend(['Training data', 'Test data'])\n",
    "#plt.savefig('UMAP_of_test_data_wgs_filt.pdf', dpi=1200) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9af25-f675-48e5-8b75-c89e7932fb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a batch of data points\n",
    "batch_size =3000  # Choose an appropriate batch size\n",
    "z_dim = 64  # Specify the dimensionality of the latent space\n",
    "z = torch.randn(batch_size, z_dim)  # Generate random latent variables\n",
    "\n",
    "# Decode the latent variables to obtain the corresponding data points\n",
    "with torch.no_grad():\n",
    "    decoded = model.decode(z)  # Decode latent variables to data points\n",
    "\n",
    "# Convert the decoded data points to numpy array\n",
    "decoded_np = decoded.numpy()\n",
    "\n",
    "# Apply UMAP to reduce the dimensionality of the latent space\n",
    "latent_umap = reducer.transform(decoded_np)\n",
    "\n",
    "# Plot the latent space reconstructions using a scatter plot\n",
    "plt.scatter(transformed_true[:, 0], transformed_true[:, 1],  color='red', alpha=0.1)\n",
    "plt.scatter(latent_umap[:, 0], latent_umap[:, 1], color='blue', alpha=0.1,)\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('UMAP of points sampled from latent space')\n",
    "plt.legend(['Training data','Generated'])\n",
    "#plt.savefig('UMAP_reconstr_train_wgs_filt_wide_mse.pdf', dpi=1200) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad339ff7-3e35-46ae-afa9-68ecae73b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "### check inter-species correlations\n",
    "cormat=stats.spearmanr(labels, axis=0).statistic ### better to calculate sparse correlations but oh well\n",
    "np.fill_diagonal(cormat, 0, wrap=False)\n",
    "print(np.min(cormat))\n",
    "print(np.max(cormat))\n",
    "plt.imshow(cormat, cmap='seismic', interpolation='nearest')\n",
    "#plt.savefig('feature_correlations_spearman_wgs.pdf', dpi=1200) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542ad31-9845-4ad9-a706-ffb3dc57a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cormat.flatten(), bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67732ca7-8ac6-4eca-90bf-fe0cfc5fbe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3818a9-a261-4e3e-b7fc-c5ba391d40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "####### RMSE loss function\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "        \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
