{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dee9be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "# from skbio.stats.composition import clr\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'mps')\n",
    "print(device)\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.family'] = 'Arial'\n",
    "mpl.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a9a17",
   "metadata": {},
   "source": [
    "#### Preprocess synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d9c0d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 samples, 64 families\n",
      "10000 samples, 79 families\n",
      "10000 samples, 88 families\n",
      "10000 samples, 92 families\n",
      "10000 samples, 95 families\n"
     ]
    }
   ],
   "source": [
    "def save_preprocessed(p_data, name_data):\n",
    "    data_df=pickle.load(open(p_data, \"rb\"))\n",
    "    idx, data_df = data_df.iloc[:, :1], data_df.iloc[:, 1:]\n",
    "\n",
    "    # filter the data and remove low abundance species\n",
    "    min_thres = 1e-5\n",
    "    data_df[data_df<=min_thres]=0\n",
    "    n_samples = data_df.shape[0]\n",
    "    data_df = data_df.loc[:, (data_df>0).sum(axis=0) >= 0.05*n_samples]\n",
    "    \n",
    "    idx = idx.loc[data_df.sum(axis=1) > 0]\n",
    "    data_df = data_df.loc[data_df.sum(axis=1) > 0]\n",
    "#     print(list(data_df.sum(axis=1)).count(0))\n",
    "\n",
    "    ## normalize to relative abundance\n",
    "    data_df = data_df.div(data_df.sum(axis=1), axis=0)\n",
    "    \n",
    "    ## separate train and test\n",
    "    features = (data_df>0).values.astype(float)\n",
    "    labels = data_df.values\n",
    "    print(f\"{features.shape[0]} samples, {features.shape[1]} families\") \n",
    "    \n",
    "    ## save preprocessed data\n",
    "    torch.save({\"features\":features, \"labels\":labels, \"names\":list(data_df.columns)}, f\"../data/{name_data}_filtered.pt\")\n",
    "\n",
    "hubs = [1, 3, 6, 12, 24]\n",
    "for hub in hubs:\n",
    "    save_preprocessed(f\"../synthetic_data/data_v4/trophic_{hub}_all_diets.pkl\", f\"synthetic_v4_trophic_{hub}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befe379e-cb18-4916-943d-290597df8468",
   "metadata": {},
   "source": [
    "### Load, split and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "092e86dc-0bd8-4be0-92f8-f97105eec8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filtered_data(p_data):\n",
    "    features = torch.load(p_data)[\"features\"]\n",
    "    labels = torch.load(p_data)[\"labels\"]\n",
    "    spc_names = torch.load(p_data)[\"names\"]\n",
    "    \n",
    "    original_indices = np.arange(len(features))\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        original_indices,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "    )\n",
    "    X_train = features[train_indices]\n",
    "    X_test = features[test_indices]\n",
    "    y_train = labels[train_indices]\n",
    "    y_test = labels[test_indices]\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    ## clr transformation for outputs\n",
    "    zero_thr = 1e-8\n",
    "    gmean_train = (np.exp(np.nansum(np.log(y_train[y_train > 0]+zero_thr)) / np.size(y_train)))\n",
    "    y_train_clr = np.log((y_train+zero_thr)/gmean_train)\n",
    "    y_test_clr = np.log((y_test+zero_thr)/gmean_train)\n",
    "\n",
    "    ## rescale the data\n",
    "    scaler = preprocessing.MaxAbsScaler().fit(y_train_clr)\n",
    "    # scaler = preprocessing.RobustScaler().fit(y_train_clr)\n",
    "    y_train_scaled = scaler.transform(y_train_clr)\n",
    "    y_test_scaled = scaler.transform(y_test_clr)\n",
    "\n",
    "    ## transform to tensors\n",
    "    X_train_scaled=torch.from_numpy(X_train).float()\n",
    "    y_train_scaled=torch.from_numpy(y_train_scaled).float()\n",
    "    X_test_scaled=torch.from_numpy(X_test).float()\n",
    "    y_test_scaled=torch.from_numpy(y_test_scaled).float()\n",
    "\n",
    "#     plt.hist(y_test_scaled.numpy().flatten(), bins=40)\n",
    "#     plt.yscale(\"log\")\n",
    "#     plt.xlabel(\"Abundance score\")\n",
    "#     plt.ylabel(\"Counts\")\n",
    "#     plt.show()\n",
    "    \n",
    "    # keep the spc names and train/test split for evaluation use\n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, {\"spcs\":spc_names,\n",
    "                                                                          \"train_idx\": train_indices, \n",
    "                                                                          \"test_idx\": test_indices}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe319596-7739-437b-9580-dc961cc66b90",
   "metadata": {},
   "source": [
    "### Apply autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0956445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAETrainer:\n",
    "    def __init__(self, model, train_loader, test_loader, optimizer, weights, device=\"cuda:0\"):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.optimizer = optimizer\n",
    "        self.weights = weights\n",
    "        self.device = device\n",
    "\n",
    "    def train_one_epoch(self, zero_thr):\n",
    "        self.model.train()\n",
    "        total_loss, total_recon_loss, total_bce_loss, total_kl_loss = 0, 0, 0, 0\n",
    "        total_acc = 0\n",
    "        total_len = 0\n",
    "        for batch in self.train_loader:\n",
    "            batch['Features'], batch['Labels'] = batch['Features'].to(device), batch['Labels'].to(device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "#             recon_x, loss, recon_loss, kl_loss = compute_loss(self.model, batch['Features'], batch['Labels'], self.weights)\n",
    "            b, nb, loss, recon_loss, bce_loss, kl_loss = compute_loss_2(self.model, batch['Features'], batch['Labels'], self.weights)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_bce_loss += bce_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            \n",
    "#             out = ((recon_x.detach())>zero_thr).float()\n",
    "            out = (b.detach()>0.5).float()\n",
    "            total_acc += (out == batch['Features']).float().sum().item() # for binary-abundance\n",
    "            total_len += batch[\"Features\"].shape[0]\n",
    "\n",
    "        return total_loss / total_len, \\\n",
    "               total_recon_loss / total_len, \\\n",
    "               total_bce_loss / total_len, \\\n",
    "               total_kl_loss / total_len, \\\n",
    "               total_acc / total_len / batch[\"Features\"].shape[1]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_one_epoch(self, zero_thr):\n",
    "        self.model.eval()\n",
    "        total_loss, total_recon_loss, total_bce_loss, total_kl_loss = 0, 0, 0, 0\n",
    "        total_acc = 0\n",
    "        total_len = 0\n",
    "        for batch in self.test_loader:\n",
    "            batch['Features'], batch['Labels'] = batch['Features'].to(device), batch['Labels'].to(device)\n",
    "            \n",
    "#             recon_x, loss, recon_loss, kl_loss = compute_loss(self.model, batch['Features'], batch['Labels'], self.weights)\n",
    "            b, nb, loss, recon_loss, bce_loss, kl_loss = compute_loss_2(self.model, batch['Features'], batch['Labels'], self.weights)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_bce_loss += bce_loss.item()\n",
    "            total_kl_loss += kl_loss.item()\n",
    "            \n",
    "#             out = ((recon_x.detach())>zero_thr).float()\n",
    "            out = (b.detach()>0.5).float()\n",
    "            total_acc += (out == batch['Features']).float().sum().item() # for binary-abundance\n",
    "            total_len += batch[\"Features\"].shape[0]\n",
    "\n",
    "        return total_loss / total_len, \\\n",
    "               total_recon_loss / total_len, \\\n",
    "               total_bce_loss / total_len, \\\n",
    "               total_kl_loss / total_len, \\\n",
    "               total_acc / total_len / batch[\"Features\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c73b6dda-73af-4315-8e9d-6b299cc2c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAE_on_data(p_data, p_output, weights=[1.0, 1.0, 0.0]):\n",
    "    '''\n",
    "    weights: recon abundance loss; presence bce loss; KL divergence. \n",
    "    '''\n",
    "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, annotations = load_filtered_data(p_data)\n",
    "    spc_names = annotations[\"spcs\"]\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, features, labels, device=None):\n",
    "            self.labels = labels\n",
    "            self.features = features\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "        def __getitem__(self, idx):\n",
    "            label = self.labels[idx]\n",
    "            data = self.features[idx]\n",
    "            return {\"Features\": data, \"Labels\": label}\n",
    "    Train = CustomDataset(X_train_scaled, y_train_scaled)\n",
    "    Test = CustomDataset(X_test_scaled, y_test_scaled)\n",
    "\n",
    "    input_dim = len(spc_names)\n",
    "    hidden_dim = 2048\n",
    "    latent_dim = 512\n",
    "    epochs = 100\n",
    "    lr = 7e-4\n",
    "    weight_decay = 3e-5\n",
    "    zero_thr = -0.8\n",
    "    \n",
    "#     # early stopping\n",
    "#     patience = 10 \n",
    "#     min_delta = 1e-3\n",
    "#     best_loss = float('inf')\n",
    "#     epochs_no_improve = 0\n",
    "#     best_model_state = None\n",
    "\n",
    "    model = VAE_2(input_dim, latent_dim, hidden_dim).to(device)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    ## create batch spits of data\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
    "    train_DS = DataLoader(Train, batch_size=100, shuffle=True, drop_last=False, **kwargs)\n",
    "    test_DS = DataLoader(Test, batch_size=100, shuffle=True, drop_last=False, **kwargs)\n",
    "\n",
    "    trainer = VAETrainer(model, train_DS, test_DS, optimizer, weights, device)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_acc = []\n",
    "    test_acc = []\n",
    "    MSEs, BCEs, KLDs = [], [], []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, MSE, BCE, KLD, acc = trainer.train_one_epoch(zero_thr)\n",
    "        train_losses.append(train_loss)\n",
    "        train_acc.append(acc)\n",
    "\n",
    "        test_loss, MSE, BCE, KLD, acc = trainer.test_one_epoch(zero_thr)\n",
    "        test_losses.append(test_loss)\n",
    "        test_acc.append(acc)\n",
    "        MSEs.append(MSE)\n",
    "        BCEs.append(BCE)\n",
    "        KLDs.append(KLD)\n",
    "        \n",
    "    torch.save({\"model\":model.state_dict(), \"annotations\":annotations}, p_output)        \n",
    "        \n",
    "    ## plot the training process\n",
    "    train_acc=np.array(train_acc)\n",
    "    test_acc = np.array(test_acc)\n",
    "    plt.plot(range(train_acc.shape[0]), train_acc*100, c='blue', label = \"train_acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy, %\")\n",
    "    plt.ylim([0, 100])\n",
    "    #     plt.xlim([0,1000])  # adjust the right leaving left unchanged\n",
    "    plt.plot(range(test_acc.shape[0]), test_acc*100, c='red', label = \"test_acc\")\n",
    "    prevalence=np.sum(X_train_scaled.numpy(), axis=0)/X_train_scaled.shape[0]\n",
    "    prevalence[prevalence>=0.5]=1\n",
    "    prevalence[prevalence<0.5]=0\n",
    "    # horizontal line showing dumb predictions based on prevalence\n",
    "    acc=(1-np.sum(np.abs(X_test_scaled.numpy()-prevalence))/X_test_scaled.shape[0]/prevalence.shape[0])*100\n",
    "    plt.axhline(y = acc, color = 'orange', linestyle = '-.')\n",
    "    plt.legend(frameon=False)\n",
    "    handle = p_data.split(\"/\")[-1][:-3]\n",
    "    plt.savefig(f\"../figures/trainingproc_acc_{handle}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # show the losses\n",
    "    lists = [MSEs, BCEs, KLDs]\n",
    "    names = [\"MSE\", \"Binary\", \"KLD\"]\n",
    "    for (yloss, loss_name) in zip(lists, names):\n",
    "        plt.plot(range(train_acc.shape[0]), yloss, label=loss_name)\n",
    "    plt.legend(frameon=False)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    handle = p_data.split(\"/\")[-1][:-3]\n",
    "    plt.savefig(f\"../figures/trainingproc_loss_{handle}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2797baf1-a472-410c-87fa-043d70607b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hub = 1\n",
    "p_data = f\"../data/synthetic_v4_trophic_{hub}_filtered.pt\"\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, annotations = load_filtered_data(p_data)\n",
    "torch.isnan(X_test_scaled).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b26ecc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 8/100 [00:09<01:53,  1.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hub \u001b[38;5;129;01min\u001b[39;00m hubs[:\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain_VAE_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/synthetic_v4_trophic_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhub\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_filtered.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/synthetic_v4_trophic_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mhub\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_trained_AE.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36mtrain_VAE_on_data\u001b[0;34m(p_data, p_output, weights)\u001b[0m\n\u001b[1;32m     53\u001b[0m MSEs, BCEs, KLDs \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 55\u001b[0m     train_loss, MSE, BCE, KLD, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzero_thr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     57\u001b[0m     train_acc\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mVAETrainer.train_one_epoch\u001b[0;34m(self, zero_thr)\u001b[0m\n\u001b[1;32m     20\u001b[0m b, nb, loss, recon_loss, bce_loss, kl_loss \u001b[38;5;241m=\u001b[39m compute_loss_2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeatures\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights)\n\u001b[1;32m     21\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m total_recon_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m recon_loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/optim/adamw.py:171\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    158\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    161\u001b[0m         group,\n\u001b[1;32m    162\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m         state_steps,\n\u001b[1;32m    169\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/optim/adamw.py:321\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 321\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/torch/lib/python3.9/site-packages/torch/optim/adamw.py:493\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    490\u001b[0m device_params \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_params]\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# update steps\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_state_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m    496\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_params, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for hub in hubs[:1]:\n",
    "    train_VAE_on_data(f\"../data/synthetic_v4_trophic_{hub}_filtered.pt\", f\"../models/synthetic_v4_trophic_{hub}_trained_AE.pt\", weights=[1.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387300c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf87316",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
